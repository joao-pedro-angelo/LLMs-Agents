# Limitações

![image](https://github.com/user-attachments/assets/1e08f2b9-2692-4d54-852c-e834e99ee054)

O artigo "Large Language Models Can Be Easily Distracted by Irrelevant Context" (2023)
explora como modelos de linguagem de grande escala (LLMs) podem ser influenciados por informações irrelevantes
presentes no contexto. 

---
## Tópicos abordados no artigo

**1. Sensibilidade a Contexto Irrelevante:**<br>
   Os LLMs, mesmo treinados em tarefas complexas, apresentam alta susceptibilidade a distrações quando há informações irrelevantes nos prompts. Isso sugere que os modelos não possuem um entendimento completo do contexto, mas frequentemente se baseiam em padrões ou correlações fracas.

**2. Impacto nas Predições:**<br>
   A presença de informações irrelevantes pode diminuir significativamente a precisão dos modelos, pois essas distrações interferem na identificação das partes relevantes do problema.

**3. Técnicas de Mitigação:**<br>
  - Decodificação com Consistência Interna: Utilizar métodos como "self-consistency", que combinam diferentes caminhos de raciocínio para chegar a respostas mais robustas.
  - Instruções no Prompt: Incluir instruções explícitas pedindo ao modelo que ignore informações irrelevantes pode melhorar a precisão.

---
## Exemplos 

![image](https://github.com/user-attachments/assets/5229cf71-d922-4291-9671-9e884ae2577a)

![image](https://github.com/user-attachments/assets/335ba9c3-93fc-4869-a33a-ea245c582fc1)

